{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Forecast: predicting time-series at scale\n",
    "\n",
    "Forecasting is used in a variety of applications and business use cases: For example, retailers need to forecast the sales of their products to decide how much stock they need by location, Manufacturers need to estimate the number of parts required at their factories to optimize their supply chain, Businesses need to estimate their flexible workforce needs, Utilities need to forecast electricity consumption needs in order to attain an efficient energy network, and \n",
    "enterprises need to estimate their cloud infrastructure needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/amazon_forecast.png\">\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "* Step 0: [Setting up](#setup)\n",
    " * Step 0a: [Snowflake Data Load](#snowflakedataload)\n",
    "* Step 1: [Preparing the Datasets](#prepare)\n",
    " * Step 1a: [Snowflake Unload to s3](#unloads3)\n",
    "* Step 2: [Importing the Data](#import)\n",
    " * Step 2a: [Creating a Dataset Group](#create)\n",
    " * Step 2b: [Creating a Target Dataset](#target)\n",
    " * Step 2c: [Creating a Related Dataset](#related)\n",
    " * Step 2d: [Update the Dataset Group](#update)\n",
    " * Step 2e: [Creating a Target Time Series Dataset Import Job](#targetImport)\n",
    " * Step 2f: [Creating a Related Time Series Dataset Import Job](#relatedImport)\n",
    "* Step 3: [Choosing an Algorithm and Evaluating its Performance](#algo)\n",
    " * Step 3a: [Choosing DeepAR+](#DeepAR)\n",
    " * Step 3b: [Choosing Prophet](#prophet)\n",
    "* Step 4: [Computing Error Metrics from Backtesting](#error)\n",
    "* Step 5: [Creating a Forecast](#forecast)\n",
    "* Step 6: [Querying the Forecasts](#query)\n",
    "* Step 7: [Exporting the Forecasts](#export)\n",
    " * Step 7a: [Load Results to Snowflake from S3](#loadoutput)\n",
    "* Step 8: [Clearning up your Resources](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  First let us setup Amazon Forecast<a class=\"anchor\" id=\"setup\">\n",
    "\n",
    "This section sets up the permissions and relevant endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and install below librarires. If it is first time installing, then you have to restart the Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade snowflake-connector-python\n",
    "!{sys.executable} -m pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../common\") )\n",
    "import util\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the S3 bucket name and region name for this lesson.\n",
    "\n",
    "- If you don't have an S3 bucket, create it first on S3.\n",
    "- Although we have set the region to us-west-2 as a default value below, you can choose any of the regions that the service is available in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget_bucket = util.create_text_widget( \"bucket_name\", \"input your S3 bucket name\" )\n",
    "text_widget_region = util.create_text_widget( \"region\", \"input region name.\", default_value=\"us-west-2\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = text_widget_bucket.value\n",
    "assert bucket_name, \"bucket_name not set.\"\n",
    "\n",
    "region = text_widget_region.value\n",
    "assert region, \"region not set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region) \n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the role to provide to Amazon Forecast.\n",
    "role_name = \"ForecastNotebookRole-RelatedTimeSeries\"\n",
    "role_arn = util.get_or_create_iam_role( role_name = role_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "<img src=\"images/outline.png\">\n",
    "\n",
    "<img src=\"images/forecast_workflow.png\">\n",
    "\n",
    "The above figure summarizes the key workflow of using Forecast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake Data Load <a class=\"anchor\" id=\"snowflakedataload\">\n",
    "\n",
    "Following section is used for illustrative purpose of connecting and loading data to Snowflake. Loading data from Snowflake to S3 and vice versa.\n",
    "\n",
    "Define snowflake connection details and connect.\n",
    "\n",
    "Following Snowflake links might be useful.\n",
    "\n",
    "[Connect to Snowflake](https://docs.snowflake.com/en/user-guide/python-connector-api.html)\n",
    "[Load data from file](https://docs.snowflake.com/en/user-guide/data-load-local-file-system-stage.html)\n",
    "[Select data from table load to pandas data frame](https://docs.snowflake.com/en/user-guide/python-connector-api.html#label-python-connector-api-fetch-pandas-all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get credentials from env file. The credentials were setup as a dictionary, so will use JSON to load.\n",
    "credentials = None\n",
    "with open('../../env/dev/credentials') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE PROCESSOR PARAMETERS\n",
    "snowflake_warehouse = 'FORECAST_WH_TEST'\n",
    "snowflake_database=  'FORECAST_DATA'\n",
    "snowflake_schema = 'FORECAST_DATA'\n",
    "snowflake_table_data = 'FORECAST_RAW_DATA'\n",
    "snowflake_forecast_results = 'FORECAST_RESULTS'\n",
    "snowflake_stage = 'FORECAST_STAGE'\n",
    "snowflake_s3 = 's3://'+bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowflake_connect(credentials, warehouse):\n",
    "    \n",
    "    # Check and make sure the credentials were pulled correctly\n",
    "    if credentials:\n",
    "        # Connect to snowflake\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=credentials.get('snowflake_user'),\n",
    "            password=credentials.get('snowflake_password'),\n",
    "            account=credentials.get('snowflake_account')\n",
    "        )\n",
    "        \n",
    "        # Establish a cursor and execute a query\n",
    "        cur = ctx.cursor()\n",
    "        cur.execute('USE ROLE SYSADMIN')\n",
    "        cur.execute('USE WAREHOUSE ' + warehouse)\n",
    "        \n",
    "        return ctx.cursor()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_initial_dataset(credentials, snowflake_warehouse, \n",
    "                          snowflake_database, snowflake_schema, \n",
    "                          snowflake_table_data):\n",
    "    #SETUP INITIAL DATASET\n",
    "    try:\n",
    "        cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "        if cursor != None:\n",
    "            cursor.execute('CREATE OR REPLACE DATABASE ' + snowflake_database)\n",
    "            cursor.execute('CREATE OR REPLACE SCHEMA ' + snowflake_schema)\n",
    "            cursor.execute('CREATE OR REPLACE TABLE ' + snowflake_table_data + ' (datetime timestamp, season int,'\n",
    "                           +' holiday int, workingday int, weather int, temp float, atemp float, humidity int, '\n",
    "                           +'windspeed int, casual int, registered int, count float) '\n",
    "                           +\" STAGE_FILE_FORMAT = ( TYPE = 'csv' FIELD_DELIMITER= ',' skip_header = 1)\")\n",
    "            cursor.execute('PUT '+'file://data/train.csv @%' + snowflake_table_data)\n",
    "            cursor.execute('COPY INTO ' + snowflake_table_data)\n",
    "    finally:\n",
    "        # Always close connections if there is an error\n",
    "        if cursor != None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_initial_dataset(credentials, snowflake_warehouse, \n",
    "                      snowflake_database, snowflake_schema, snowflake_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preparing the Datasets<a class=\"anchor\" id=\"prepare\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "    if cursor != None:\n",
    "        # Execute a statement that will generate a result set.\n",
    "        sql = \"select * from \"+ snowflake_database + '.' + snowflake_schema +'.' + snowflake_table_data\n",
    "        cursor.execute(sql)\n",
    "        # Fetch the result set from the cursor and deliver it as the Pandas DataFrame.\n",
    "        bike_df = cursor.fetch_pandas_all()\n",
    "        #bike_df = pd.read_csv(\"data/train.csv\", dtype = object)\n",
    "        bike_df.columns = ['datetime','season','holiday','workingday',\n",
    "                       'weather','temp','atemp','humidity','windspeed','casual','registered','count']\n",
    "        display(bike_df.head())\n",
    "finally:\n",
    "    # Always close connections if there is an error\n",
    "    if cursor != None:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df['count'] = bike_df['count'].astype('float')\n",
    "bike_df['workingday'] = bike_df['workingday'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take about two and a half week's of hourly data for demonstration, just for the purpose that there's no missing data in the whole range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_small = bike_df[-2*7*24-24*3:].copy()\n",
    "bike_df_small['item_id'] = \"bike_12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the time series first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_small.plot(x='datetime', y='count', figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target time series seem to have a drop over weekends. Next let's plot both the target time series and the related time series that indicates whether today is a `workday` or not. More precisely, $r_t = 1$ if $t$ is a work day and 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "bike_df_small.plot(x='datetime', y='count', ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "bike_df_small.plot(x='datetime', y='workingday', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that to use the related time series, we need to ensure that the related time series covers the whole target time series, as well as the future values as specified by the forecast horizon. More precisely, we need to make sure:\n",
    "```\n",
    "len(related time series) >= len(target time series) + forecast horizon\n",
    "```\n",
    "Basically, all items need to have data start at or before the item start date, and have data until the forecast horizon (i.e. the latest end date across all items + forecast horizon).  Additionally, there should be no missing values in the related time series. The following picture illustrates the desired logic. \n",
    "\n",
    "<img src=\"images/rts_viz.png\">\n",
    "\n",
    "For more details regarding how to prepare your Related Time Series dataset, please refer to the public documentation <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html\">here</a>. \n",
    "\n",
    "Suppose in this particular example, we wish to forecast for the next 24 hours, and thus we generate the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = bike_df_small[['item_id', 'datetime', 'count']][:-24]\n",
    "rts_df = bike_df_small[['item_id', 'datetime', 'workingday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the length of the related time series is equal to the length of the target time series plus the forecast horizon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target_df), len(rts_df))\n",
    "assert len(target_df) + 24 == len(rts_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check whether there are \"holes\" in the related time series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rts_df) == len(pd.date_range(\n",
    "    start=list(rts_df['datetime'])[0],\n",
    "    end=list(rts_df['datetime'])[-1],\n",
    "    freq='H'\n",
    ")), \"missing entries in the related time series\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, and we plot both time series again. As it can be seen, the related time series (indicator of whether the current day is a workday or not) is longer than the target time series.  The binary working day indicator feature is a good example of a related time series, since it is known at all future time points.  Other examples of related time series include holiday and promotion features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "ax = plt.gca()\n",
    "target_df.plot(x='datetime', y='count', ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "rts_df.plot(x='datetime', y='workingday', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv(\"data/bike_small.csv\", index= False, header = False)\n",
    "rts_df.to_csv(\"data/bike_small_rts.csv\", index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake unload to s3<a class=\"anchor\" id=\"unloads3\">\n",
    "In this step we load the target time series and related time series data into Snowflake tables.\n",
    "Note that the stage tables created with s3 urls, will unload data to s3 directly using s3 credentials provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_tts = 'FORECAST_TTS'\n",
    "snowflake_tts_stage = 'FORECAST_TTS_STAGE'\n",
    "\n",
    "\n",
    "def setup_tts_dataset_unload_s3(credentials, snowflake_warehouse,snowflake_database, \n",
    "                                snowflake_schema, snowflake_tts, snowflake_tts_stage,snowflake_s3):\n",
    "    try:\n",
    "        cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "        if cursor != None:\n",
    "            cursor.execute('USE DATABASE ' + snowflake_database)\n",
    "            cursor.execute('USE SCHEMA ' + snowflake_schema)\n",
    "            cursor.execute('CREATE OR REPLACE TABLE ' + snowflake_tts + ' (item_id string, datetime timestamp, count float) '\n",
    "                           +\" STAGE_FILE_FORMAT = ( TYPE = 'csv' FIELD_DELIMITER= ',')\")\n",
    "            cursor.execute('PUT '+'file://data/bike_small.csv @%' + snowflake_tts)\n",
    "            cursor.execute('COPY INTO ' + snowflake_tts)\n",
    "            cursor.execute(\"CREATE OR REPLACE file format my_csv_format type = csv field_delimiter = ',' compression = NONE TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS' \")\n",
    "            cursor.execute('CREATE OR REPLACE STAGE ' + snowflake_tts_stage + \" file_format = my_csv_format\"\n",
    "                           +' url=\\''+ snowflake_s3 + '\\' credentials=(aws_key_id=\\'' + credentials.get('aws_access_key_id') \n",
    "                           + '\\' aws_secret_key=\\'' + credentials.get('aws_secret_access_key') + '\\' )')\n",
    "            cursor.execute('COPY INTO @' + snowflake_tts_stage + '/bike_small/tts/ FROM (select * FROM ' \n",
    "                           + snowflake_database + '.' + snowflake_schema +'.' + snowflake_tts + ') OVERWRITE = TRUE')\n",
    "    \n",
    "    finally:\n",
    "        # Always close connections if there is an error\n",
    "        if cursor != None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tts_dataset_unload_s3(credentials, snowflake_warehouse,snowflake_database, \n",
    "                            snowflake_schema, snowflake_tts, snowflake_tts_stage, snowflake_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_rts = 'FORECAST_RTS'\n",
    "snowflake_rts_stage = 'FORECAST_RTS_STAGE'\n",
    "\n",
    "\n",
    "def setup_rts_dataset_unload_s3(credentials, snowflake_warehouse,snowflake_database, \n",
    "                                snowflake_schema, snowflake_rts, snowflake_rts_stage, snowflake_s3):\n",
    "    #SETUP INITIAL DATASET\n",
    "    try:\n",
    "        cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "        if cursor != None:\n",
    "            cursor.execute('USE DATABASE ' + snowflake_database)\n",
    "            cursor.execute('USE SCHEMA ' + snowflake_schema)\n",
    "            cursor.execute('CREATE OR REPLACE TABLE ' + snowflake_rts + ' (item_id string, datetime timestamp, workingday float) '\n",
    "                           +\" STAGE_FILE_FORMAT = ( TYPE = 'csv' FIELD_DELIMITER= ',')\")\n",
    "            cursor.execute('PUT '+'file://data/bike_small_rts.csv @%' + snowflake_rts)\n",
    "            cursor.execute('COPY INTO ' + snowflake_rts)\n",
    "            #cursor.execute(\"CREATE OR REPLACE file format my_csv_format type = csv field_delimiter = ',' compression = NONE\")\n",
    "            cursor.execute('CREATE OR REPLACE STAGE ' + snowflake_rts_stage + \" file_format = my_csv_format\"\n",
    "                           +' url=\\''+ snowflake_s3 + '\\' credentials=(aws_key_id=\\'' + credentials.get('aws_access_key_id')  \n",
    "                           + '\\' aws_secret_key=\\'' + credentials.get('aws_secret_access_key') + '\\' )')\n",
    "            cursor.execute('COPY INTO @' + snowflake_rts_stage + '/bike_small/rts/ FROM (select * FROM ' \n",
    "                           + snowflake_database + '.' + snowflake_schema +'.' + snowflake_rts + ') OVERWRITE = TRUE')\n",
    "    finally:\n",
    "        # Always close connections if there is an error\n",
    "        if cursor != None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_rts_dataset_unload_s3(credentials, snowflake_warehouse, snowflake_database, \n",
    "                            snowflake_schema, snowflake_rts, snowflake_rts_stage, snowflake_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client('s3')\n",
    "key = \"bike_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Importing the Data<a class=\"anchor\" id=\"import\">\n",
    "\n",
    "\n",
    "Now we are ready to import the datasets into the Forecast service. Starting from the raw data, Amazon Forecast automatically extracts the dataset that is suitable for forecasting. As an example, a retailer normally records the transaction record such as\n",
    "<img src=\"images/data_format.png\">\n",
    "<img src=\"images/timestamp.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "import string\n",
    "\n",
    "project = 'bike_rts_demo_{}'.format(''.join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f\"s3://{bucket_name}/{key}\"\n",
    "print(s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we specify key input data and forecast parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"H\"\n",
    "forecast_horizon = 24\n",
    "timestamp_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "delimiter = ','"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a. Creating a Dataset Group<a class=\"anchor\" id=\"create\">\n",
    "First let's create a dataset group and then update it later to add our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group = f\"{project}_gp\"\n",
    "dataset_arns = []\n",
    "create_dataset_group_response = forecast.create_dataset_group(Domain=\"RETAIL\",\n",
    "                                                          DatasetGroupName=dataset_group,\n",
    "                                                          DatasetArns=dataset_arns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating dataset group {dataset_group}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b. Creating a Target Dataset<a class=\"anchor\" id=\"target\">\n",
    "In this example, we will define a target time series. This is a required dataset to use the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we specify the target time series name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_name = f\"{project}_ts\"\n",
    "print(ts_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the schema of our dataset below. Make sure the order of the attributes (columns) matches the raw data in the files. We follow the same three attribute format as the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_schema_val = [{\"AttributeName\": \"item_id\", \"AttributeType\": \"string\"},\n",
    "              {\"AttributeName\": \"timestamp\", \"AttributeType\": \"timestamp\"},\n",
    "              {\"AttributeName\": \"demand\", \"AttributeType\": \"float\"}]\n",
    "ts_schema = {\"Attributes\": ts_schema_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating target dataset {ts_dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                               DatasetType='TARGET_TIME_SERIES',\n",
    "                               DatasetName=ts_dataset_name,\n",
    "                               DataFrequency=freq,\n",
    "                               Schema=ts_schema\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_arn = response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=ts_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c. Creating a Related Dataset<a class=\"anchor\" id=\"related\">\n",
    "In this example, we will define a related time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the related time series name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_name = f\"{project}_rts\"\n",
    "print(rts_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the schema of your dataset here. Make sure the order of columns matches the raw data files. We follow the same three column format as the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_schema_val = [{\"AttributeName\": \"item_id\", \"AttributeType\": \"string\"},\n",
    "              {\"AttributeName\": \"timestamp\", \"AttributeType\": \"timestamp\"},\n",
    "              {\"AttributeName\": \"workingday\", \"AttributeType\": \"float\"}]\n",
    "rts_schema = {\"Attributes\": rts_schema_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating related dataset {rts_dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                               DatasetType='RELATED_TIME_SERIES',\n",
    "                               DatasetName=rts_dataset_name,\n",
    "                               DataFrequency=freq,\n",
    "                               Schema=rts_schema\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_arn = response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=rts_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2d. Updating the dataset group with the datasets we created<a class=\"anchor\" id=\"update\">\n",
    "You can have multiple datasets under the same dataset group. Update it with the datasets we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "dataset_arns.append(ts_dataset_arn)\n",
    "dataset_arns.append(rts_dataset_arn)\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2e. Creating a Target Time Series Dataset Import Job<a class=\"anchor\" id=\"targetImport\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_s3_data_path = f\"{s3_data_path}/tts/data_0_0_0.csv\"\n",
    "print (ts_s3_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_import_job_response = forecast.create_dataset_import_job(DatasetImportJobName=dataset_group,\n",
    "                                                             DatasetArn=ts_dataset_arn,\n",
    "                                                             DataSource= {\n",
    "                                                                 \"S3Config\" : {\n",
    "                                                                     \"Path\": ts_s3_data_path,\n",
    "                                                                     \"RoleArn\": role_arn\n",
    "                                                                 } \n",
    "                                                             },\n",
    "                                                             TimestampFormat=timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_import_job_arn=ts_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2f. Creating a Related Time Series Dataset Import Job<a class=\"anchor\" id=\"relatedImport\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_s3_data_path = f\"{s3_data_path}/rts/data_0_0_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_import_job_response = forecast.create_dataset_import_job(DatasetImportJobName=dataset_group,\n",
    "                                                             DatasetArn=rts_dataset_arn,\n",
    "                                                             DataSource= {\n",
    "                                                                 \"S3Config\" : {\n",
    "                                                                     \"Path\": rts_s3_data_path,\n",
    "                                                                     \"RoleArn\": role_arn\n",
    "                                                                 } \n",
    "                                                             },\n",
    "                                                             TimestampFormat=timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_import_job_arn=rts_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Choosing an algorithm and evaluating its performance<a class=\"anchor\" id=\"algo\">\n",
    "\n",
    "Once the datasets are specified with the corresponding schema, Amazon Forecast will automatically aggregate all the relevant pieces of information for each item, such as sales, price, promotions, as well as categorical attributes, and generate the desired dataset. Next, one can choose an algorithm (forecasting model) and evaluate how well this particular algorithm works on this dataset. The following graph gives a high-level overview of the forecasting models.\n",
    "<img src=\"images/recipes.png\">\n",
    "<img src=\"images/pred_details.png\">\n",
    "\n",
    "\n",
    "Amazon Forecast provides several state-of-the-art forecasting algorithms including classic forecasting methods such as ETS, ARIMA, Prophet and deep learning approaches such as DeepAR+. Classical forecasting methods, such as Autoregressive Integrated Moving Average (ARIMA) or Exponential Smoothing (ETS), fit a single model to each individual time series, and then use that model to extrapolate the time series into the future. Amazon's Non-Parametric Time Series (NPTS) forecaster also fits a single model to each individual time series.  Unlike the naive or seasonal naive forecasters that use a fixed time index (the previous index $T-1$ or the past season $T - \\tau$) as the prediction for time step $T$, NPTS randomly samples a time index $t \\in \\{0, \\dots T-1\\}$ in the past to generate a sample for the current time step $T$.\n",
    "\n",
    "In many applications, you may encounter many similar time series across a set of cross-sectional units. Examples of such time series groupings are demand for different products, server loads, and requests for web pages. In this case, it can be beneficial to train a single model jointly over all of these time series. DeepAR+ takes this approach, outperforming the standard ARIMA and ETS methods when your dataset contains hundreds of related time series. The trained model can also be used for generating forecasts for new time series that are similar to the ones it has been trained on. While deep learning approaches can outperform standard methods, this is only possible when there is sufficient data available for training. It is not true for example when one trains a neural network with a time-series contains only a few dozens of observations. Amazon Forecast provides the best of two worlds allowing users to either choose a specific algorithm or let Amazon Forecast automatically perform model selection. \n",
    "\n",
    "## How to evaluate a forecasting model?\n",
    "\n",
    "Before moving forward, let's first introduce the notion of *backtest* when evaluating forecasting models. The key difference between evaluating forecasting algorithms and standard ML applications is that we need to make sure there is no future information gets used in the past. In other words, the procedure needs to be causal. \n",
    "\n",
    "<img src=\"images/backtest.png\">\n",
    "\n",
    "\n",
    "In this notebook, let's compare the neural network based method, DeepAR+ with Facebook's open-source Bayesian method Prophet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a. Choosing DeepAR+<a class=\"anchor\" id=\"DeepAR\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'Deep_AR_Plus'\n",
    "algorithm_arn_deep_ar_plus = algorithm_arn + algorithm\n",
    "predictor_name_deep_ar = f'{project}_{algorithm.lower()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[{predictor_name_deep_ar}] Creating predictor {predictor_name_deep_ar} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_predictor_response = forecast.create_predictor(PredictorName=predictor_name_deep_ar,\n",
    "                                                  AlgorithmArn=algorithm_arn_deep_ar_plus,\n",
    "                                                  ForecastHorizon=forecast_horizon,\n",
    "                                                  PerformAutoML=False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": dataset_group_arn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": freq}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_arn_deep_ar = create_predictor_response['PredictorArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b.  Choosing Prophet<a class=\"anchor\" id=\"prophet\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'Prophet'\n",
    "algorithm_arn_prophet = algorithm_arn + algorithm\n",
    "predictor_name_prophet = f'{project}_{algorithm.lower()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[{predictor_name_prophet}] Creating predictor %s ...' % predictor_name_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_predictor_response = forecast.create_predictor(PredictorName=predictor_name_prophet,\n",
    "                                                  AlgorithmArn=algorithm_arn_prophet,\n",
    "                                                  ForecastHorizon=forecast_horizon,\n",
    "                                                  PerformAutoML=False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": dataset_group_arn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": freq}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_arn_prophet = create_predictor_response['PredictorArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_prophet))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_predictor(PredictorArn=predictor_arn_prophet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Computing Error Metrics from Backtesting<a class=\"anchor\" id=\"error\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the predictors, we can query the forecast accuracy given by the backtest scenario and have a quantitative understanding of the performance of the algorithm. Such a process is iterative in nature during model development. When an algorithm with satisfying performance is found, the customer can deploy the predictor into a production environment, and query the forecasts for a particular item to make business decisions. The figure below shows a sample plot of different quantile forecasts of a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done creating predictor. Getting accuracy numbers for DeepAR+ ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_deep_ar)\n",
    "error_metrics_deep_ar_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done creating predictor. Getting accuracy numbers for Prophet ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_prophet = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_prophet)\n",
    "error_metrics_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "prophet_metrics = extract_summary_metrics(error_metrics_prophet, \"Prophet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([deep_ar_metrics, prophet_metrics]) \\\n",
    "    .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, if you only have a handful of time series (in this case, only 1) with a small number of examples, the neural network models (DeepAR+) are not the best choice. Here, we clearly see that DeepAR+ behaves worse than Prophet in the case of a single time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Creating a Forecast<a class=\"anchor\" id=\"forecast\">\n",
    "\n",
    "Next we re-train with the full dataset, and create the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Done fetching accuracy numbers. Creating forecaster for DeepAR+ ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name_deep_ar = f'{project}_deep_ar_plus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response_deep_ar = forecast.create_forecast(ForecastName=forecast_name_deep_ar,\n",
    "                                                        PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn_deep_ar = create_forecast_response_deep_ar['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Done fetching accuracy numbers. Creating forecaster for Prophet ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name_prophet = f'{project}_prophet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response_prophet = forecast.create_forecast(ForecastName=forecast_name_prophet,\n",
    "                                                        PredictorArn=predictor_arn_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn_prophet = create_forecast_response_prophet['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_prophet))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_forecast(ForecastArn=forecast_arn_prophet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Querying the Forecasts<a class=\"anchor\" id=\"query\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 'bike_12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_response_deep = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Filters={\"item_id\": item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_response_prophet = forecast_query.query_forecast(ForecastArn=forecast_arn_prophet,\n",
    "                                                     Filters={\"item_id\":item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'data/bike_small.csv'\n",
    "exact = util.load_exact_sol(fname, item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_forecasts(forecast_response_deep, exact)\n",
    "plt.title(\"DeepAR Forecast\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_forecasts(forecast_response_prophet,exact)\n",
    "plt.title(\"Prophet Forecast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Exporting your Forecasts<a class=\"anchor\" id=\"export\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_export_name_deep_ar = f'{project}_forecast_export_deep_ar_plus'\n",
    "forecast_export_name_deep_ar_path = f\"{s3_data_path}/{forecast_export_name_deep_ar}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_export_response_deep_ar = forecast.create_forecast_export_job(ForecastExportJobName=forecast_export_name_deep_ar,\n",
    "                                                        ForecastArn=forecast_arn_deep_ar,\n",
    "                                                        Destination={\n",
    "                                                            \"S3Config\" : {\n",
    "                                                                \"Path\": forecast_export_name_deep_ar_path,\n",
    "                                                                \"RoleArn\": role_arn\n",
    "                                                            }\n",
    "                                                        })\n",
    "forecast_export_arn_deep_ar = create_forecast_export_response_deep_ar['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast_export_job(ForecastExportJobArn = forecast_export_arn_deep_ar))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_export_name_prophet = f'{project}_forecast_export_prophet'\n",
    "forecast_export_name_prophet_path = f\"{s3_data_path}/{forecast_export_name_prophet}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_export_response_prophet = forecast.create_forecast_export_job(ForecastExportJobName=forecast_export_name_prophet,\n",
    "                                                        ForecastArn=forecast_arn_prophet,\n",
    "                                                        Destination={\n",
    "                                                            \"S3Config\" : {\n",
    "                                                                \"Path\": forecast_export_name_prophet_path,\n",
    "                                                                \"RoleArn\": role_arn\n",
    "                                                            }\n",
    "                                                        })\n",
    "forecast_export_arn_prophet = create_forecast_export_response_prophet['ForecastExportJobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast_export_job(ForecastExportJobArn = forecast_export_arn_prophet))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results to Snowflake from S3<a class='anchor' id='loadoutput' />\n",
    "\n",
    "Setup output tables to store the output details in snowflake db. Note that, stage tables used copies data from s3path to snowflake output tables with s3 credentials provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_db_schema(credentials, snowflake_warehouse, snowflake_database, snowflake_schema):\n",
    "    \n",
    "    try:\n",
    "        cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "        if cursor != None:\n",
    "            cursor.execute('CREATE OR REPLACE DATABASE ' + snowflake_database)\n",
    "            cursor.execute('CREATE OR REPLACE SCHEMA ' + snowflake_schema)    \n",
    "    finally:\n",
    "        # Always close connections if there is an error\n",
    "        if cursor != None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_output_db_schema(credentials, snowflake_warehouse, snowflake_forecast_results, snowflake_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_results_deep_ar_plus = 'FORECAST_RESULTS_DEEP_AR_PLUS'\n",
    "snowflake_results_deep_ar_plus_stage = 'FORECAST_RESULTS_DEEP_AR_PLUS_STAGE'\n",
    "\n",
    "\n",
    "def setup_output_table(credentials, snowflake_warehouse, snowflake_database, \n",
    "                       snowflake_schema, snowflake_output, snowflake_output_stage, snowflake_s3):\n",
    "    \n",
    "    try:\n",
    "        cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "        if cursor != None:\n",
    "            cursor.execute('USE DATABASE ' + snowflake_database)\n",
    "            cursor.execute('USE SCHEMA ' + snowflake_schema)\n",
    "            cursor.execute('CREATE OR REPLACE STAGE ' + snowflake_output_stage + ' url=\\'' + snowflake_s3 + '\\' credentials=(aws_key_id=\\'' + credentials.get('aws_access_key_id') + '\\' aws_secret_key=\\'' + credentials.get('aws_secret_access_key') + '\\')')\n",
    "            cursor.execute('CREATE OR REPLACE TABLE ' + snowflake_output + ' ( item_id string, date datetime, p10 float, p50 float, p90 float)')\n",
    "    \n",
    "    finally:\n",
    "        # Always close connections if there is an error\n",
    "        if cursor != None:\n",
    "            cursor.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_output_table(credentials, snowflake_warehouse, snowflake_forecast_results, snowflake_schema, \n",
    "                   snowflake_results_deep_ar_plus, snowflake_results_deep_ar_plus_stage, snowflake_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_results_prophet = 'FORECAST_RESULTS_PROPHET'\n",
    "snowflake_results_prophet_stage = 'FORECAST_RESULTS_PROPHET_STAGE'\n",
    "\n",
    "setup_output_table(credentials, snowflake_warehouse, snowflake_forecast_results, \n",
    "                   snowflake_schema, snowflake_results_prophet, snowflake_results_prophet_stage, snowflake_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_output_snowflake(credentials, snowflake_warehouse, s3_path,\n",
    "                         snowflake_database, snowflake_schema, snowflake_output, snowflake_output_stage):\n",
    "    try:\n",
    "        cursor = snowflake_connect(credentials, snowflake_warehouse)\n",
    "        if cursor != None:\n",
    "            cursor.execute('USE DATABASE ' + snowflake_database)\n",
    "            cursor.execute('USE SCHEMA ' + snowflake_schema)\n",
    "            cursor.execute('COPY INTO ' + snowflake_output + ' FROM @' + snowflake_output_stage \n",
    "                           + '/'+s3_path+' file_format=(skip_header=1)')\n",
    "    finally:\n",
    "        # Always close connections if there is an error\n",
    "        if cursor != None:\n",
    "            cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure to use the actual path of forecast of deep ar plus export\n",
    "s3_path_deep_ar_plus = 'bike_small/<forecast_deep_arplus_export_path>'\n",
    "load_output_snowflake(credentials, snowflake_warehouse, s3_path_deep_ar_plus,\n",
    "                         snowflake_forecast_results, snowflake_schema, snowflake_results_deep_ar_plus, \n",
    "                      snowflake_results_deep_ar_plus_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure to use the actual path of forecast of prophet export\n",
    "s3_path_prophet = 'bike_small/<forecast_prephet_export_path>'\n",
    "load_output_snowflake(credentials, snowflake_warehouse, s3_path_prophet,\n",
    "                         snowflake_forecast_results, snowflake_schema, snowflake_results_prophet, \n",
    "                      snowflake_results_prophet_stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Cleaning up your Resources<a class=\"anchor\" id=\"cleanup\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have completed the above steps, we can start to cleanup the resources we created. All delete jobs, except for `delete_dataset_group` are asynchronous, so we have added the helpful `wait_till_delete` function. \n",
    "Resource Limits documented <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/limits.html\">here</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete forecast export for both algorithms\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn_deep_ar))\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn_prophet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete forecast for both algorithms\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn_deep_ar))\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn_prophet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete predictor for both algorithms\n",
    "util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn = predictor_arn_deep_ar))\n",
    "util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn = predictor_arn_prophet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the target time series and related time series dataset import jobs\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the target time series and related time series datasets\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=ts_dataset_arn))\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=rts_dataset_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataset group\n",
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete IAM role\n",
    "util.delete_iam_role( role_name )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
